{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level language modelling\n",
    "Based on [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehot, argmax, chunk, batchseq, throttle, crossentropy\n",
    "using StatsBase: wsample\n",
    "using Base.Iterators: partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load text data from `input.txt` and split it into characters, then turn it into the numeric form needed by the model.\n",
    "\n",
    "The model will take a sequence of characters, like \"the do\", and try to produce the next character (e.g. 't' or 'g' would be likely here but not 'd'). The target output sequence $Y$ is therefore just the input sequence $X$ offset by one, e.g.\n",
    "\n",
    "* $X$: `the dog`\n",
    "* $Y$: `he dog_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = collect(readstring(\"data/input.txt\"))\n",
    "alphabet = [unique(text)..., '_']\n",
    "text = map(ch -> onehot(ch, alphabet), text)\n",
    "stop = onehot('_', alphabet)\n",
    "\n",
    "N = length(alphabet)\n",
    "seqlen = 50\n",
    "nbatch = 50\n",
    "\n",
    "Xs = collect(partition(batchseq(chunk(text, nbatch), stop), seqlen))\n",
    "Ys = collect(partition(batchseq(chunk(text[2:end], nbatch), stop), seqlen));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be a multi-layer LSTM, which takes a single character as input and produces a single character as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "  LSTM(N, 128),\n",
    "  LSTM(128, 128),\n",
    "  Dense(128, N),\n",
    "  softmax)\n",
    "\n",
    "m = gpu(m)\n",
    "\n",
    "predict(x) = m(gpu(collect(x)))\n",
    "\n",
    "function loss(xs, ys)\n",
    "  l = sum(crossentropy.(predict.(xs), gpu.(ys)))\n",
    "  Flux.truncate!(m)\n",
    "  return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accepts a one-hot-encoded character and returns a probability distribution over possible subsequent characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = predict(onehot('a', alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from this distribution to see what the model thinks comes after 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsample(alphabet, probabilities.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we feed the model's output back into itself, we can allow it to \"dream\" a sequence of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample(m, alphabet, len; temp = 1)\n",
    "  Flux.reset!(m)\n",
    "  buf = IOBuffer()\n",
    "  c = rand('a':'z')\n",
    "  for i = 1:len\n",
    "    write(buf, c)\n",
    "    c = wsample(alphabet, m(gpu(collect(onehot(c, alphabet)))).data)\n",
    "  end\n",
    "  return String(take!(buf))\n",
    "end\n",
    "\n",
    "sample(m, alphabet, 100) |> println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now it's more-or-less random because the model hasn't seen any data. Let's fix that.\n",
    "\n",
    "We just need to call `Flux.train!` with an optimiser and the data we prepared. We set up a call back so that every 30 seconds, we get to see a sample of the model's output, which you should see learning a basic words and grammar fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = ADAM(params(m))\n",
    "evalcb = function ()\n",
    "  print_with_color(:blue, \"Loss is $(loss(Xs[5], Ys[5]))\\n\")\n",
    "  println(sample(deepcopy(m), alphabet, 500))\n",
    "end\n",
    "Flux.train!(loss, zip(Xs, Ys), opt, cb = throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2-pre",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
